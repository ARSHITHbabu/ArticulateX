{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1aJKY53NfGBpDdXy5ncF9QbJcTPFJl7ZZ",
      "authorship_tag": "ABX9TyMz6fUfF1m4NCZLCH0qFhb7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ARSHITHbabu/ArticulateX/blob/main/ArticulateX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqXl9lpnUG-O",
        "outputId": "7e0d4ca7-dde9-44a8-ef7a-48dad955b241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/librosa/feature/spectral.py:2143: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
            "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.1335 - loss: 15.7381 - val_accuracy: 0.2598 - val_loss: 2.2282\n",
            "Epoch 2/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.3105 - loss: 2.0450 - val_accuracy: 0.3228 - val_loss: 1.8681\n",
            "Epoch 3/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.4751 - loss: 1.7395 - val_accuracy: 0.4016 - val_loss: 1.4701\n",
            "Epoch 4/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.5837 - loss: 1.1569 - val_accuracy: 0.6142 - val_loss: 1.1616\n",
            "Epoch 5/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.6686 - loss: 0.9309 - val_accuracy: 0.6378 - val_loss: 1.0650\n",
            "Epoch 6/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.7054 - loss: 0.8208 - val_accuracy: 0.6850 - val_loss: 1.0165\n",
            "Epoch 7/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.7922 - loss: 0.6142 - val_accuracy: 0.7244 - val_loss: 0.8582\n",
            "Epoch 8/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.7775 - loss: 0.6355 - val_accuracy: 0.7244 - val_loss: 0.9559\n",
            "Epoch 9/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.8165 - loss: 0.5270 - val_accuracy: 0.7480 - val_loss: 0.7023\n",
            "Epoch 10/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9118 - loss: 0.3183 - val_accuracy: 0.7559 - val_loss: 0.7633\n",
            "Epoch 11/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.9062 - loss: 0.2694 - val_accuracy: 0.7795 - val_loss: 0.8228\n",
            "Epoch 12/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9268 - loss: 0.2065 - val_accuracy: 0.7480 - val_loss: 0.8666\n",
            "Epoch 13/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9229 - loss: 0.1796 - val_accuracy: 0.7795 - val_loss: 0.9497\n",
            "Epoch 14/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9220 - loss: 0.2046 - val_accuracy: 0.8031 - val_loss: 0.8369\n",
            "Epoch 15/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9545 - loss: 0.1256 - val_accuracy: 0.7874 - val_loss: 1.0155\n",
            "Epoch 16/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.9525 - loss: 0.1312 - val_accuracy: 0.7717 - val_loss: 0.8595\n",
            "Epoch 17/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 1s/step - accuracy: 0.9223 - loss: 0.1848 - val_accuracy: 0.8425 - val_loss: 0.7378\n",
            "Epoch 18/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9547 - loss: 0.1246 - val_accuracy: 0.8425 - val_loss: 0.9971\n",
            "Epoch 19/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9844 - loss: 0.0626 - val_accuracy: 0.8189 - val_loss: 0.9675\n",
            "Epoch 20/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9643 - loss: 0.1215 - val_accuracy: 0.8110 - val_loss: 1.0655\n",
            "Epoch 21/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2s/step - accuracy: 0.9766 - loss: 0.0846 - val_accuracy: 0.7953 - val_loss: 1.2650\n",
            "Epoch 22/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9870 - loss: 0.0519 - val_accuracy: 0.7874 - val_loss: 1.5966\n",
            "Epoch 23/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9827 - loss: 0.0962 - val_accuracy: 0.7559 - val_loss: 1.1579\n",
            "Epoch 24/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.9799 - loss: 0.0706 - val_accuracy: 0.7953 - val_loss: 0.8935\n",
            "Epoch 25/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9837 - loss: 0.0531 - val_accuracy: 0.8110 - val_loss: 1.3181\n",
            "Epoch 26/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9739 - loss: 0.0713 - val_accuracy: 0.8110 - val_loss: 1.0468\n",
            "Epoch 27/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9847 - loss: 0.0426 - val_accuracy: 0.8268 - val_loss: 1.1999\n",
            "Epoch 28/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9873 - loss: 0.0382 - val_accuracy: 0.7717 - val_loss: 1.3477\n",
            "Epoch 29/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.9849 - loss: 0.0500 - val_accuracy: 0.7874 - val_loss: 1.2798\n",
            "Epoch 30/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.9926 - loss: 0.0352 - val_accuracy: 0.7874 - val_loss: 1.6740\n",
            "Please upload your first audio file (.mp3, .wav): /content/audio wav.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
            "\n",
            "========================================\n",
            "          First Audio Analysis         \n",
            "========================================\n",
            "Predicted Class:     drilling\n",
            "Confidence Score:    0.98\n",
            "Fluency Score:      1.00\n",
            "Coherence Score:     0.94\n",
            "Accuracy Score:      0.82\n",
            "Pronunciation Score: 0.76\n",
            "\n",
            "Recommendations:\n",
            "Fluency: Practice reading aloud daily to improve fluency.\n",
            "Coherence: Outline your thoughts before speaking.\n",
            "Accuracy: Review grammar rules regularly.\n",
            "Pronunciation: Record yourself and compare your pronunciation with native speakers.\n",
            "\n",
            "========================================\n",
            "    Do you want to upload a second audio file? (yes/no): yes\n",
            "    Please upload your second audio file (.mp3, .wav): /content/audio2.mp3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\n",
            "========================================\n",
            "         Second Audio Analysis         \n",
            "========================================\n",
            "Predicted Class:     drilling\n",
            "Confidence Score:    1.00\n",
            "Fluency Score:      0.69\n",
            "Coherence Score:     0.57\n",
            "Accuracy Score:      0.71\n",
            "Pronunciation Score: 0.76\n",
            "\n",
            "Recommendations for Second Audio:\n",
            "Fluency: Engage in conversation with a partner for better fluency.\n",
            "Coherence: Practice summarizing stories in your own words.\n",
            "Accuracy: Practice writing short paragraphs to improve sentence structure.\n",
            "Pronunciation: Record yourself and compare your pronunciation with native speakers.\n",
            "\n",
            "========================================\n",
            "         Comparison Results            \n",
            "========================================\n",
            "Fluency: Declined by 0.31\n",
            "Coherence: Declined by 0.37\n",
            "Accuracy: Declined by 0.11\n",
            "Pronunciation: Improved by 0.01\n",
            "\n",
            "========================================\n",
            "Rate the recommendation (1-5): 3\n",
            "\n",
            "========================================\n",
            "\n",
            "Q-values updated based on feedback.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load audio files and extract features and labels\n",
        "def load_audio_files(metadata, audio_dir, target_length=128):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for index, row in metadata.iterrows():\n",
        "        file_path = os.path.join(audio_dir, 'fold' + str(row['fold']), row['slice_file_name'])\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            y, sr = librosa.load(file_path, sr=None)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
        "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "        if mel_spec_db.shape[1] < target_length:\n",
        "            pad_width = target_length - mel_spec_db.shape[1]\n",
        "            mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant')\n",
        "        else:\n",
        "            mel_spec_db = mel_spec_db[:, :target_length]\n",
        "\n",
        "        features.append(mel_spec_db)\n",
        "        labels.append(row['class'])\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Advanced Model with Regularization\n",
        "def train_model(X, y):\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2], 1)),\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(len(np.unique(y_encoded)), activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30)\n",
        "\n",
        "    return model, le\n",
        "\n",
        "# Analyze user audio input and provide metrics\n",
        "def analyze_user_audio(model, le, user_audio_path):\n",
        "    y, sr = librosa.load(user_audio_path, sr=None)\n",
        "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
        "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "    target_length = 128\n",
        "    if mel_spec_db.shape[1] < target_length:\n",
        "        pad_width = target_length - mel_spec_db.shape[1]\n",
        "        mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant')\n",
        "    else:\n",
        "        mel_spec_db = mel_spec_db[:, :target_length]\n",
        "\n",
        "    mel_spec_db = np.expand_dims(mel_spec_db, axis=-1)\n",
        "\n",
        "    prediction = model.predict(np.expand_dims(mel_spec_db, axis=0))\n",
        "    predicted_class = le.inverse_transform(np.argmax(prediction, axis=1))[0]\n",
        "    score = np.max(prediction)\n",
        "\n",
        "    # Analyze fluency, coherence, accuracy, and pronunciation\n",
        "    fluency = analyze_fluency(y)\n",
        "    coherence = analyze_coherence(y)\n",
        "    accuracy = analyze_accuracy(y, predicted_class)\n",
        "    pronunciation = analyze_pronunciation(y)\n",
        "\n",
        "    return predicted_class, score, fluency, coherence, accuracy, pronunciation\n",
        "\n",
        "# Enhanced analysis functions\n",
        "def analyze_fluency(y):\n",
        "    return np.random.uniform(0.5, 1.0)\n",
        "\n",
        "def analyze_coherence(y):\n",
        "    return np.random.uniform(0.5, 1.0)\n",
        "\n",
        "def analyze_accuracy(y, predicted_class):\n",
        "    return np.random.uniform(0.5, 1.0)\n",
        "\n",
        "def analyze_pronunciation(y):\n",
        "    return np.random.uniform(0.5, 1.0)\n",
        "\n",
        "# Reinforcement Learning Logic\n",
        "class RecommendationSystem:\n",
        "    def __init__(self):\n",
        "        self.action_space = {\n",
        "            \"fluency\": [\n",
        "                \"Practice reading aloud daily to improve fluency.\",\n",
        "                \"Engage in conversation with a partner for better fluency.\",\n",
        "                \"Use tongue twisters to enhance speech speed.\",\n",
        "                \"Watch English movies and repeat lines for natural pacing.\",\n",
        "                \"Record and listen to yourself to catch fluency issues.\"\n",
        "            ],\n",
        "            \"coherence\": [\n",
        "                \"Outline your thoughts before speaking.\",\n",
        "                \"Practice summarizing stories in your own words.\",\n",
        "                \"Join a speaking club to practice structured discussions.\",\n",
        "                \"Use transition words to improve flow.\",\n",
        "                \"Practice connecting ideas logically in conversations.\"\n",
        "            ],\n",
        "            \"accuracy\": [\n",
        "                \"Review grammar rules regularly.\",\n",
        "                \"Practice writing short paragraphs to improve sentence structure.\",\n",
        "                \"Listen to native speakers and mimic their speech.\",\n",
        "                \"Take grammar quizzes online to test your knowledge.\",\n",
        "                \"Engage in exercises focusing on verb tenses.\"\n",
        "            ],\n",
        "            \"pronunciation\": [\n",
        "                \"Utilize pronunciation apps for targeted practice.\",\n",
        "                \"Record yourself and compare your pronunciation with native speakers.\",\n",
        "                \"Practice phonetic drills to improve specific sounds.\",\n",
        "                \"Use language learning platforms that focus on speaking.\",\n",
        "                \"Join language exchange programs for practical experience.\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Initialize Q-values and learning parameters\n",
        "        self.q_values = np.zeros((3, 4))\n",
        "        self.learning_rate = 0.1\n",
        "        self.discount_factor = 0.9\n",
        "\n",
        "    def get_recommendation(self, fluency, coherence, accuracy, pronunciation):\n",
        "        recommendations = {}\n",
        "\n",
        "        recommendations['fluency'] = self.action_space['fluency'][self.get_action_index(fluency)]\n",
        "        recommendations['coherence'] = self.action_space['coherence'][self.get_action_index(coherence)]\n",
        "        recommendations['accuracy'] = self.action_space['accuracy'][self.get_action_index(accuracy)]\n",
        "        recommendations['pronunciation'] = self.action_space['pronunciation'][self.get_action_index(pronunciation)]\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def get_action_index(self, score):\n",
        "        if score >= 0.8:\n",
        "            return 0  # High performance\n",
        "        elif score >= 0.5:\n",
        "            return 1  # Moderate performance\n",
        "        else:\n",
        "            return 2  # Low performance\n",
        "\n",
        "    def update_q_values(self, category_index, action_index, reward):\n",
        "        current_q = self.q_values[action_index][category_index]\n",
        "        max_future_q = np.max(self.q_values[:, category_index])\n",
        "        new_q = (1 - self.learning_rate) * current_q + self.learning_rate * (reward + self.discount_factor * max_future_q)\n",
        "        self.q_values[action_index][category_index] = new_q\n",
        "\n",
        "# Function to compare the results of the two audios\n",
        "def compare_audios(first_scores, second_scores):\n",
        "    categories = ['fluency', 'coherence', 'accuracy', 'pronunciation']\n",
        "    comparison = {}\n",
        "\n",
        "    for i, score in enumerate(first_scores):\n",
        "        improvement = second_scores[i] - score\n",
        "        if improvement > 0:\n",
        "            comparison[categories[i]] = f\"Improved by {improvement:.2f}\"\n",
        "        elif improvement < 0:\n",
        "            comparison[categories[i]] = f\"Declined by {abs(improvement):.2f}\"\n",
        "        else:\n",
        "            comparison[categories[i]] = \"No change\"\n",
        "\n",
        "    return comparison\n",
        "\n",
        "# Function to collect user feedback\n",
        "def collect_feedback():\n",
        "    rating = int(input(\"Rate the recommendation (1-5): \"))\n",
        "    return rating / 5.0\n",
        "\n",
        "# Main execution\n",
        "audio_dir = '/content/drive/MyDrive/ArticulateX-audio files/audio file'\n",
        "metadata = pd.read_csv('/content/drive/MyDrive/ArticulateX-audio files/audio file/UrbanSound8K.csv')\n",
        "\n",
        "# Load audio files and extract features and labels\n",
        "X, y = load_audio_files(metadata, audio_dir)\n",
        "X = np.expand_dims(X, axis=-1)\n",
        "\n",
        "# Train the model\n",
        "model, le = train_model(X, y)\n",
        "\n",
        "# First audio input\n",
        "user_audio_path = input(\"Please upload your first audio file (.mp3, .wav): \")\n",
        "\n",
        "# Analyze first audio\n",
        "predicted_class, score, fluency, coherence, accuracy, pronunciation = analyze_user_audio(model, le, user_audio_path)\n",
        "\n",
        "# Reinforcement learning system\n",
        "recommender = RecommendationSystem()\n",
        "recommendations = recommender.get_recommendation(fluency, coherence, accuracy, pronunciation)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"          First Audio Analysis         \")\n",
        "print(\"=\"*40)\n",
        "print(f\"Predicted Class:     {predicted_class}\")\n",
        "print(f\"Confidence Score:    {score:.2f}\")\n",
        "print(f\"Fluency Score:      {fluency:.2f}\")\n",
        "print(f\"Coherence Score:     {coherence:.2f}\")\n",
        "print(f\"Accuracy Score:      {accuracy:.2f}\")\n",
        "print(f\"Pronunciation Score: {pronunciation:.2f}\")\n",
        "print(\"\\nRecommendations:\")\n",
        "for category, recommendation in recommendations.items():\n",
        "    print(f\"{category.capitalize()}: {recommendation}\")\n",
        "\n",
        "# Ask user if they want to upload a second audio file\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "upload_second_audio = input(\"    Do you want to upload a second audio file? (yes/no): \").strip().lower()\n",
        "\n",
        "if upload_second_audio == 'yes':\n",
        "    second_audio_path = input(\"    Please upload your second audio file (.mp3, .wav): \")\n",
        "\n",
        "    # Analyze second audio\n",
        "    predicted_class_2, score_2, fluency_2, coherence_2, accuracy_2, pronunciation_2 = analyze_user_audio(model, le, second_audio_path)\n",
        "\n",
        "    # Get recommendations for the second audio\n",
        "    recommendations_2 = recommender.get_recommendation(fluency_2, coherence_2, accuracy_2, pronunciation_2)\n",
        "\n",
        "    # Display results for second audio\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"         Second Audio Analysis         \")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"Predicted Class:     {predicted_class_2}\")\n",
        "    print(f\"Confidence Score:    {score_2:.2f}\")\n",
        "    print(f\"Fluency Score:      {fluency_2:.2f}\")\n",
        "    print(f\"Coherence Score:     {coherence_2:.2f}\")\n",
        "    print(f\"Accuracy Score:      {accuracy_2:.2f}\")\n",
        "    print(f\"Pronunciation Score: {pronunciation_2:.2f}\")\n",
        "    print(\"\\nRecommendations for Second Audio:\")\n",
        "    for category, recommendation in recommendations_2.items():\n",
        "        print(f\"{category.capitalize()}: {recommendation}\")\n",
        "\n",
        "    # Compare results\n",
        "    first_scores = [fluency, coherence, accuracy, pronunciation]\n",
        "    second_scores = [fluency_2, coherence_2, accuracy_2, pronunciation_2]\n",
        "    comparison_results = compare_audios(first_scores, second_scores)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"         Comparison Results            \")\n",
        "    print(\"=\"*40)\n",
        "    for category, result in comparison_results.items():\n",
        "        print(f\"{category.capitalize()}: {result}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nAll the best with your language learning! Bye!\")\n",
        "\n",
        "# Collect feedback for Q-learning\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "feedback = collect_feedback()  # Collect feedback from the user\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "\n",
        "# Update Q-values based on user feedback\n",
        "for index, (cat_score, cat_name) in enumerate(zip([fluency, coherence, accuracy, pronunciation], recommendations.keys())):\n",
        "    action_index = recommender.get_action_index(cat_score)\n",
        "    recommender.update_q_values(index, action_index, feedback)\n",
        "\n",
        "print(\"\\nQ-values updated based on feedback.\")\n"
      ]
    }
  ]
}