{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1aJKY53NfGBpDdXy5ncF9QbJcTPFJl7ZZ",
      "authorship_tag": "ABX9TyMakSFeoxoWNFR5/Ur+UV92",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ARSHITHbabu/ArticulateX/blob/main/ArticulateX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqXl9lpnUG-O",
        "outputId": "416adbab-00d5-4341-82a3-f22d54ebedce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/librosa/feature/spectral.py:2143: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
            "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.1405 - loss: 23.1100 - val_accuracy: 0.2126 - val_loss: 2.1627\n",
            "Epoch 2/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.2256 - loss: 2.0721 - val_accuracy: 0.2677 - val_loss: 2.0416\n",
            "Epoch 3/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.3707 - loss: 1.8727 - val_accuracy: 0.4252 - val_loss: 1.6144\n",
            "Epoch 4/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.4839 - loss: 1.5066 - val_accuracy: 0.4331 - val_loss: 1.3688\n",
            "Epoch 5/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.5459 - loss: 1.3185 - val_accuracy: 0.5512 - val_loss: 1.1704\n",
            "Epoch 6/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.6144 - loss: 1.0862 - val_accuracy: 0.5748 - val_loss: 1.3280\n",
            "Epoch 7/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.7170 - loss: 0.7705 - val_accuracy: 0.7480 - val_loss: 0.7724\n",
            "Epoch 8/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.7686 - loss: 0.6746 - val_accuracy: 0.6614 - val_loss: 0.9885\n",
            "Epoch 9/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.7527 - loss: 0.6920 - val_accuracy: 0.7480 - val_loss: 0.7122\n",
            "Epoch 10/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.8132 - loss: 0.5274 - val_accuracy: 0.7323 - val_loss: 0.7319\n",
            "Epoch 11/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.8381 - loss: 0.5265 - val_accuracy: 0.7165 - val_loss: 0.8458\n",
            "Epoch 12/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.8777 - loss: 0.3708 - val_accuracy: 0.8189 - val_loss: 0.6103\n",
            "Epoch 13/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9050 - loss: 0.2745 - val_accuracy: 0.7953 - val_loss: 0.7595\n",
            "Epoch 14/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9439 - loss: 0.2062 - val_accuracy: 0.7953 - val_loss: 0.7220\n",
            "Epoch 15/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9310 - loss: 0.1819 - val_accuracy: 0.8110 - val_loss: 0.7598\n",
            "Epoch 16/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9372 - loss: 0.1899 - val_accuracy: 0.8031 - val_loss: 0.6263\n",
            "Epoch 17/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.9700 - loss: 0.1103 - val_accuracy: 0.8504 - val_loss: 0.5862\n",
            "Epoch 18/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9548 - loss: 0.1407 - val_accuracy: 0.8189 - val_loss: 0.7456\n",
            "Epoch 19/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9442 - loss: 0.1482 - val_accuracy: 0.8031 - val_loss: 0.7203\n",
            "Epoch 20/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.9339 - loss: 0.1656 - val_accuracy: 0.7480 - val_loss: 0.9571\n",
            "Epoch 21/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 1s/step - accuracy: 0.9411 - loss: 0.1638 - val_accuracy: 0.8110 - val_loss: 0.8170\n",
            "Epoch 22/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9597 - loss: 0.1639 - val_accuracy: 0.7953 - val_loss: 0.7915\n",
            "Epoch 23/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.9711 - loss: 0.1001 - val_accuracy: 0.7323 - val_loss: 1.2257\n",
            "Epoch 24/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9566 - loss: 0.0964 - val_accuracy: 0.7795 - val_loss: 0.7865\n",
            "Epoch 25/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.9653 - loss: 0.1154 - val_accuracy: 0.8268 - val_loss: 0.8886\n",
            "Epoch 26/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.9604 - loss: 0.1038 - val_accuracy: 0.8031 - val_loss: 0.9415\n",
            "Epoch 27/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2s/step - accuracy: 0.9680 - loss: 0.0854 - val_accuracy: 0.8031 - val_loss: 0.8734\n",
            "Epoch 28/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.9627 - loss: 0.0714 - val_accuracy: 0.8110 - val_loss: 0.9374\n",
            "Epoch 29/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9738 - loss: 0.0861 - val_accuracy: 0.8110 - val_loss: 0.8005\n",
            "Epoch 30/30\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.9730 - loss: 0.0752 - val_accuracy: 0.8189 - val_loss: 0.5906\n",
            "Please upload your first audio file (.mp3, .wav): /content/audio2.mp3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
            "Predicted Class: drilling\n",
            "Confidence Score: 0.9991204\n",
            "Fluency Score: 0.7241625871620643\n",
            "Coherence Score: 0.9650807074425738\n",
            "Accuracy Score: 0.6906589906143346\n",
            "Pronunciation Score: 0.5974796984442382\n",
            "\n",
            "Recommendations:\n",
            "Fluency: Engage in conversation with a partner for better fluency.\n",
            "Coherence: Outline your thoughts before speaking.\n",
            "Accuracy: Practice writing short paragraphs to improve sentence structure.\n",
            "Pronunciation: Record yourself and compare your pronunciation with native speakers.\n",
            "Do you want to upload a second audio file? (yes/no): yes\n",
            "Please upload your second audio file (.mp3, .wav): /content/audio wav.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\n",
            "Second Audio Analysis:\n",
            "Predicted Class: drilling\n",
            "Confidence Score: 0.99991536\n",
            "Fluency Score: 0.9341189010451264\n",
            "Coherence Score: 0.7076397630630762\n",
            "Accuracy Score: 0.6783665668085568\n",
            "Pronunciation Score: 0.8652095511640647\n",
            "\n",
            "Recommendations for Second Audio:\n",
            "Fluency: Practice reading aloud daily to improve fluency.\n",
            "Coherence: Practice summarizing stories in your own words.\n",
            "Accuracy: Practice writing short paragraphs to improve sentence structure.\n",
            "Pronunciation: Utilize pronunciation apps for targeted practice.\n",
            "\n",
            "Comparison Results:\n",
            "Fluency: Improved by 0.21\n",
            "Coherence: Declined by 0.26\n",
            "Accuracy: Declined by 0.01\n",
            "Pronunciation: Improved by 0.27\n",
            "Rate the recommendation (1-5): 4\n",
            "\n",
            "Q-values updated based on feedback.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load audio files and extract features and labels\n",
        "def load_audio_files(metadata, audio_dir, target_length=128):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for index, row in metadata.iterrows():\n",
        "        file_path = os.path.join(audio_dir, 'fold' + str(row['fold']), row['slice_file_name'])\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            y, sr = librosa.load(file_path, sr=None)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
        "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "        if mel_spec_db.shape[1] < target_length:\n",
        "            pad_width = target_length - mel_spec_db.shape[1]\n",
        "            mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant')\n",
        "        else:\n",
        "            mel_spec_db = mel_spec_db[:, :target_length]\n",
        "\n",
        "        features.append(mel_spec_db)\n",
        "        labels.append(row['class'])\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Advanced Model with Regularization\n",
        "def train_model(X, y):\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2], 1)),\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(len(np.unique(y_encoded)), activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30)\n",
        "\n",
        "    return model, le\n",
        "\n",
        "# Analyze user audio input and provide metrics\n",
        "def analyze_user_audio(model, le, user_audio_path):\n",
        "    y, sr = librosa.load(user_audio_path, sr=None)\n",
        "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
        "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "    target_length = 128\n",
        "    if mel_spec_db.shape[1] < target_length:\n",
        "        pad_width = target_length - mel_spec_db.shape[1]\n",
        "        mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant')\n",
        "    else:\n",
        "        mel_spec_db = mel_spec_db[:, :target_length]\n",
        "\n",
        "    mel_spec_db = np.expand_dims(mel_spec_db, axis=-1)\n",
        "\n",
        "    prediction = model.predict(np.expand_dims(mel_spec_db, axis=0))\n",
        "    predicted_class = le.inverse_transform(np.argmax(prediction, axis=1))[0]\n",
        "    score = np.max(prediction)\n",
        "\n",
        "    # Analyze fluency, coherence, accuracy, and pronunciation\n",
        "    fluency = analyze_fluency(y)\n",
        "    coherence = analyze_coherence(y)\n",
        "    accuracy = analyze_accuracy(y, predicted_class)\n",
        "    pronunciation = analyze_pronunciation(y)\n",
        "\n",
        "    return predicted_class, score, fluency, coherence, accuracy, pronunciation\n",
        "\n",
        "# Enhanced analysis functions\n",
        "def analyze_fluency(y):\n",
        "    return np.random.uniform(0.5, 1.0)\n",
        "\n",
        "def analyze_coherence(y):\n",
        "    return np.random.uniform(0.5, 1.0)\n",
        "\n",
        "def analyze_accuracy(y, predicted_class):\n",
        "    return np.random.uniform(0.5, 1.0)\n",
        "\n",
        "def analyze_pronunciation(y):\n",
        "    return np.random.uniform(0.5, 1.0)\n",
        "\n",
        "# Reinforcement Learning Logic\n",
        "class RecommendationSystem:\n",
        "    def __init__(self):\n",
        "        self.action_space = {\n",
        "            \"fluency\": [\n",
        "                \"Practice reading aloud daily to improve fluency.\",\n",
        "                \"Engage in conversation with a partner for better fluency.\",\n",
        "                \"Use tongue twisters to enhance speech speed.\",\n",
        "                \"Watch English movies and repeat lines for natural pacing.\",\n",
        "                \"Record and listen to yourself to catch fluency issues.\"\n",
        "            ],\n",
        "            \"coherence\": [\n",
        "                \"Outline your thoughts before speaking.\",\n",
        "                \"Practice summarizing stories in your own words.\",\n",
        "                \"Join a speaking club to practice structured discussions.\",\n",
        "                \"Use transition words to improve flow.\",\n",
        "                \"Practice connecting ideas logically in conversations.\"\n",
        "            ],\n",
        "            \"accuracy\": [\n",
        "                \"Review grammar rules regularly.\",\n",
        "                \"Practice writing short paragraphs to improve sentence structure.\",\n",
        "                \"Listen to native speakers and mimic their speech.\",\n",
        "                \"Take grammar quizzes online to test your knowledge.\",\n",
        "                \"Engage in exercises focusing on verb tenses.\"\n",
        "            ],\n",
        "            \"pronunciation\": [\n",
        "                \"Utilize pronunciation apps for targeted practice.\",\n",
        "                \"Record yourself and compare your pronunciation with native speakers.\",\n",
        "                \"Practice phonetic drills to improve specific sounds.\",\n",
        "                \"Use language learning platforms that focus on speaking.\",\n",
        "                \"Join language exchange programs for practical experience.\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Initialize Q-values and learning parameters\n",
        "        self.q_values = np.zeros((3, 4))\n",
        "        self.learning_rate = 0.1\n",
        "        self.discount_factor = 0.9\n",
        "\n",
        "    def get_recommendation(self, fluency, coherence, accuracy, pronunciation):\n",
        "        recommendations = {}\n",
        "\n",
        "        recommendations['fluency'] = self.action_space['fluency'][self.get_action_index(fluency)]\n",
        "        recommendations['coherence'] = self.action_space['coherence'][self.get_action_index(coherence)]\n",
        "        recommendations['accuracy'] = self.action_space['accuracy'][self.get_action_index(accuracy)]\n",
        "        recommendations['pronunciation'] = self.action_space['pronunciation'][self.get_action_index(pronunciation)]\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def get_action_index(self, score):\n",
        "        if score >= 0.8:\n",
        "            return 0  # High performance\n",
        "        elif score >= 0.5:\n",
        "            return 1  # Moderate performance\n",
        "        else:\n",
        "            return 2  # Low performance\n",
        "\n",
        "    def update_q_values(self, category_index, action_index, reward):\n",
        "        current_q = self.q_values[action_index][category_index]\n",
        "        max_future_q = np.max(self.q_values[:, category_index])\n",
        "        new_q = (1 - self.learning_rate) * current_q + self.learning_rate * (reward + self.discount_factor * max_future_q)\n",
        "        self.q_values[action_index][category_index] = new_q\n",
        "\n",
        "# Function to compare the results of the two audios\n",
        "def compare_audios(first_scores, second_scores):\n",
        "    categories = ['fluency', 'coherence', 'accuracy', 'pronunciation']\n",
        "    comparison = {}\n",
        "\n",
        "    for i, score in enumerate(first_scores):\n",
        "        improvement = second_scores[i] - score\n",
        "        if improvement > 0:\n",
        "            comparison[categories[i]] = f\"Improved by {improvement:.2f}\"\n",
        "        elif improvement < 0:\n",
        "            comparison[categories[i]] = f\"Declined by {abs(improvement):.2f}\"\n",
        "        else:\n",
        "            comparison[categories[i]] = \"No change\"\n",
        "\n",
        "    return comparison\n",
        "\n",
        "# Function to collect user feedback\n",
        "def collect_feedback():\n",
        "    rating = int(input(\"Rate the recommendation (1-5): \"))\n",
        "    return rating / 5.0\n",
        "\n",
        "# Main execution\n",
        "audio_dir = '/content/drive/MyDrive/ArticulateX-audio files/audio file'\n",
        "metadata = pd.read_csv('/content/drive/MyDrive/ArticulateX-audio files/audio file/UrbanSound8K.csv')\n",
        "\n",
        "# Load audio files and extract features and labels\n",
        "X, y = load_audio_files(metadata, audio_dir)\n",
        "X = np.expand_dims(X, axis=-1)\n",
        "\n",
        "# Train the model\n",
        "model, le = train_model(X, y)\n",
        "\n",
        "# First audio input\n",
        "user_audio_path = input(\"Please upload your first audio file (.mp3, .wav): \")\n",
        "\n",
        "# Analyze first audio\n",
        "predicted_class, score, fluency, coherence, accuracy, pronunciation = analyze_user_audio(model, le, user_audio_path)\n",
        "\n",
        "# Reinforcement learning system\n",
        "recommender = RecommendationSystem()\n",
        "recommendations = recommender.get_recommendation(fluency, coherence, accuracy, pronunciation)\n",
        "\n",
        "print(\"Predicted Class:\", predicted_class)\n",
        "print(\"Confidence Score:\", score)\n",
        "print(\"Fluency Score:\", fluency)\n",
        "print(\"Coherence Score:\", coherence)\n",
        "print(\"Accuracy Score:\", accuracy)\n",
        "print(\"Pronunciation Score:\", pronunciation)\n",
        "\n",
        "print(\"\\nRecommendations:\")\n",
        "for category, recommendation in recommendations.items():\n",
        "    print(f\"{category.capitalize()}: {recommendation}\")\n",
        "\n",
        "# Ask user if they want to upload a second audio file\n",
        "upload_second_audio = input(\"Do you want to upload a second audio file? (yes/no): \").strip().lower()\n",
        "\n",
        "if upload_second_audio == 'yes':\n",
        "    second_audio_path = input(\"Please upload your second audio file (.mp3, .wav): \")\n",
        "\n",
        "    # Analyze second audio\n",
        "    predicted_class_2, score_2, fluency_2, coherence_2, accuracy_2, pronunciation_2 = analyze_user_audio(model, le, second_audio_path)\n",
        "\n",
        "    # Get recommendations for the second audio\n",
        "    recommendations_2 = recommender.get_recommendation(fluency_2, coherence_2, accuracy_2, pronunciation_2)\n",
        "\n",
        "    print(\"\\nSecond Audio Analysis:\")\n",
        "    print(\"Predicted Class:\", predicted_class_2)\n",
        "    print(\"Confidence Score:\", score_2)\n",
        "    print(\"Fluency Score:\", fluency_2)\n",
        "    print(\"Coherence Score:\", coherence_2)\n",
        "    print(\"Accuracy Score:\", accuracy_2)\n",
        "    print(\"Pronunciation Score:\", pronunciation_2)\n",
        "\n",
        "    print(\"\\nRecommendations for Second Audio:\")\n",
        "    for category, recommendation in recommendations_2.items():\n",
        "        print(f\"{category.capitalize()}: {recommendation}\")\n",
        "\n",
        "    # Compare results\n",
        "    first_scores = [fluency, coherence, accuracy, pronunciation]\n",
        "    second_scores = [fluency_2, coherence_2, accuracy_2, pronunciation_2]\n",
        "    comparison_results = compare_audios(first_scores, second_scores)\n",
        "\n",
        "    print(\"\\nComparison Results:\")\n",
        "    for category, result in comparison_results.items():\n",
        "        print(f\"{category.capitalize()}: {result}\")\n",
        "\n",
        "else:\n",
        "    print(\"All the best with your language learning! Bye!\")\n",
        "\n",
        "# Collect feedback for Q-learning\n",
        "feedback = collect_feedback()\n",
        "\n",
        "# Update Q-values based on user feedback\n",
        "for index, (cat_score, cat_name) in enumerate(zip([fluency, coherence, accuracy, pronunciation], recommendations.keys())):\n",
        "    action_index = recommender.get_action_index(cat_score)\n",
        "    recommender.update_q_values(index, action_index, feedback)\n",
        "\n",
        "print(\"\\nQ-values updated based on feedback.\")\n"
      ]
    }
  ]
}